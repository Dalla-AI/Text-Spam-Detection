{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G0FCSoHnzP_S",
        "outputId": "33a04322-c782-4988-f8c0-05e19de56a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ],
      "metadata": {
        "id": "ETLBNcNzzh1l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths within your Google Drive\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "\n",
        "\n",
        "# Path to your training CSV (update the path as needed)\n",
        "# The original path was likely incorrect.\n",
        "# Assuming your CSV is within a folder called 'dataset_splits' in your Google Drive:\n",
        "train_csv_path = os.path.join(drive_path, 'dataset_splits', 'train', 'train.csv')\n",
        "\n",
        "# Load the training data\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Rename columns for clarity: assuming 'v2' is text and 'v1' is label\n",
        "train_df = train_df.rename(columns={\"sms\": \"text\", \"label\": \"label\"})\n",
        "\n",
        "# Use a subset (e.g., 200 examples) for prompt engineering\n",
        "train_subset = train_df.sample(n=200, random_state=69420)\n",
        "\n",
        "# Map numeric labels to strings for zero-shot evaluation.\n",
        "# We will use candidate labels \"yes\" (spam) and \"no\" (not spam)\n",
        "def map_label(label):\n",
        "    return \"spam\" if label == 1 else \"not spam\"\n",
        "\n",
        "train_subset['mapped_label'] = train_subset['label'].apply(map_label)\n",
        "\n",
        "# Extract texts and true labels\n",
        "texts_subset = train_subset[\"text\"].tolist()\n",
        "true_labels = train_subset[\"mapped_label\"].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGQDNkEczh93",
        "outputId": "607b49c4-fb82-4579-8a83-cabcedf5977e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New candidate labels\n",
        "candidate_labels = [\"spam\", \"not spam\"]\n",
        "\n",
        "# New prompt templates to try\n",
        "template_options = [\n",
        "    \"This message is {}.\",\n",
        "    \"Classify this text as {}.\",\n",
        "    \"The text is {}.\"\n",
        "]"
      ],
      "metadata": {
        "id": "yHPVOsvkziA2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1: facebook/bart-large-mnli\n",
        "model1_name = \"facebook/bart-large-mnli\"\n",
        "classifier_model1 = pipeline(\"zero-shot-classification\", model=model1_name, device=0)\n",
        "\n",
        "# Model 2: valhalla/distilbart-mnli-12-3\n",
        "model2_name = \"valhalla/distilbart-mnli-12-3\"\n",
        "classifier_model2 = pipeline(\"zero-shot-classification\", model=model2_name, device=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH0kB7ZWziDE",
        "outputId": "6339bf65-9061-4298-80ec-e1e8f7e11e24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(classifier, texts, candidate_labels, hypothesis_template):\n",
        "    preds = []\n",
        "    for text in texts:\n",
        "        result = classifier(text, candidate_labels, hypothesis_template=hypothesis_template)\n",
        "        # The predicted label is the first in the \"labels\" list\n",
        "        preds.append(result[\"labels\"][0])\n",
        "    return preds\n",
        "\n",
        "def evaluate_predictions(preds, true_labels):\n",
        "    acc = accuracy_score(true_labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average=\"binary\", pos_label=\"spam\")\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Evaluate each combination on the training subset\n",
        "results = {}\n",
        "\n",
        "for model_name, classifier in [(\"Model1_BART\", classifier_model1), (\"Model2_DistilBART\", classifier_model2)]:\n",
        "    for template in template_options:\n",
        "        preds = get_predictions(classifier, texts_subset, candidate_labels, hypothesis_template=template)\n",
        "        metrics = evaluate_predictions(preds, true_labels)  # true_labels should also be updated accordingly\n",
        "        results[f\"{model_name} with template: '{template}'\"] = metrics\n",
        "\n",
        "# Display the evaluation results\n",
        "for key, metrics in results.items():\n",
        "    print(key)\n",
        "    print(metrics)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCq5HsBKziGe",
        "outputId": "663d3a56-6ec0-464a-d0fd-139e56a62fd1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model1_BART with template: 'This message is {}.'\n",
            "{'accuracy': 0.875, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
            "\n",
            "Model1_BART with template: 'Classify this text as {}.'\n",
            "{'accuracy': 0.88, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
            "\n",
            "Model1_BART with template: 'The text is {}.'\n",
            "{'accuracy': 0.865, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
            "\n",
            "Model2_DistilBART with template: 'This message is {}.'\n",
            "{'accuracy': 0.88, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
            "\n",
            "Model2_DistilBART with template: 'Classify this text as {}.'\n",
            "{'accuracy': 0.88, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
            "\n",
            "Model2_DistilBART with template: 'The text is {}.'\n",
            "{'accuracy': 0.88, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data (similar to the training data)\n",
        "# Update the path to include your Google Drive path if the file is there\n",
        "test_csv_path = os.path.join(drive_path, 'dataset_splits', 'test', 'test.csv')\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "test_df = test_df.rename(columns={\"sms\": \"text\", \"label\": \"label\"})\n",
        "test_df['mapped_label'] = test_df['label'].apply(map_label)\n",
        "\n",
        "# Extract texts and true labels from the test set\n",
        "texts_test = test_df[\"text\"].tolist()\n",
        "true_labels_test = test_df[\"mapped_label\"].tolist()\n",
        "\n",
        "# Function to evaluate a zero-shot model on the test set using the chosen template\n",
        "def evaluate_on_test(classifier, hypothesis_template):\n",
        "    preds = get_predictions(classifier, texts_test, candidate_labels, hypothesis_template)\n",
        "    metrics = evaluate_predictions(preds, true_labels_test)\n",
        "    return metrics\n",
        "\n",
        "# Assume Template 1 was selected for both models; adjust if needed based on Cell 6.\n",
        "test_results_model1 = evaluate_on_test(classifier_model1, template1)\n",
        "test_results_model2 = evaluate_on_test(classifier_model2, template1)\n",
        "\n",
        "print(\"Test Set Evaluation - Model 1 (facebook/bart-large-mnli) with Template 1:\")\n",
        "print(test_results_model1)\n",
        "print(\"\\nTest Set Evaluation - Model 2 (valhalla/distilbart-mnli-12-3) with Template 1:\")\n",
        "print(test_results_model2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh2YV1okziJL",
        "outputId": "7a39f763-aa9f-43bf-a498-368528c37da4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation - Model 1 (facebook/bart-large-mnli) with Template 1:\n",
            "{'accuracy': 0.7784753363228699, 'precision': 0.15, 'recall': 0.14093959731543623, 'f1': 0.1453287197231834}\n",
            "\n",
            "Test Set Evaluation - Model 2 (valhalla/distilbart-mnli-12-3) with Template 1:\n",
            "{'accuracy': 0.831390134529148, 'precision': 0.046511627906976744, 'recall': 0.013422818791946308, 'f1': 0.020833333333333332}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Make sure you have mounted Google Drive first:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Specify the absolute path in your Google Drive where you want to save the results\n",
        "drive_save_path = \"/content/drive/MyDrive/ZeroShotResults\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# Assume these variables exist from your zero-shot experiment:\n",
        "# texts_test: List of test texts\n",
        "# preds: List of predictions from the zero-shot pipeline\n",
        "# true_labels_test: List of true labels for the test set\n",
        "\n",
        "# For demonstration purposes, here's an example dummy data:\n",
        "# texts_test = [\"Example text 1\", \"Example text 2\"]\n",
        "# preds = [\"spam\", \"not spam\"]\n",
        "# true_labels_test = [\"spam\", \"spam\"]\n",
        "\n",
        "# Generate predictions for the entire test set using your chosen model and template\n",
        "preds = get_predictions(classifier_model1, texts_test, candidate_labels, template_options[0]) # Assuming template_options[0] is your desired template\n",
        "\n",
        "# Create a DataFrame with your results\n",
        "# Now preds will have the same length as texts_test and true_labels_test\n",
        "results_df = pd.DataFrame({\n",
        "    \"text\": texts_test,\n",
        "    \"prediction\": preds,\n",
        "    \"true_label\": true_labels_test\n",
        "})\n",
        "\n",
        "# Define the full path for saving the CSV file\n",
        "csv_path = os.path.join(drive_save_path, \"zero_shot_predictions.csv\")\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "print(\"Predictions saved to\", csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTuucFNh6o2l",
        "outputId": "21141747-1ca1-481a-edd5-822a6608e5bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/drive/MyDrive/ZeroShotResults/zero_shot_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}